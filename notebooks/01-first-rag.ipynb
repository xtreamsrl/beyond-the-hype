{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Our Product: A Movie Expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "You are asked to write a chatbot to help users with choosing a movie to watch. It's a simple task, after all. You know that LLMs can store factual informations, but to have more reliable answers and up-to-date information, you choose to implement a pattern named Retrieval Augmented Generation (RAG). The term was coined in a famous paper from 2020 [(arXiv:2005.11401)](https://arxiv.org/abs/2005.11401) - at the time, ChatGPT wasn't even released, though we already had models like GPT-3 ([arXiv:2005.14165](https://arxiv.org/abs/2005.14165)) to play around with. (GPT-3 weights weren't, and still aren't, released.)\n",
    "\n",
    "After less than a week of work, we finalised our proof-of-concept (PoC). Et voil√†: here's a robotic movie expert ready to go in production and reply to all cinema geeks' questions! Let's briefly review the code implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install some code utilities\n",
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"beyond-the-hype\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/beyond-the-hype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let's start by the knowledge base. Embeddings have already been created, using `MiniLM`. If you need a refresher, you can find a brief explanation about what an embedding is and what vector databases in the first notebook. You can open it in Colab with this link: [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/movies-buddy/blob/main/notebooks/00-dataset_builder.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beyond_the_hype.data import get_movies_dataset\n",
    "\n",
    "movies = get_movies_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "To perform a semantic search, we *must* encode the query with the same model used for embedding the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We use lanceDB, an in-memory vector storage, to store our movies and vectors and build a function to retrieve movies given a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "uri = f\"../data/movies_embeddings\"\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "movies_table = db.create_table(\"movies\", movies, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "To keep things simple, we wrap the code to perform the query embedding and search in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(query, *, encoder=encoder, db_table=movies_table, max_results=10):\n",
    "    query_vector = encoder.encode(query).tolist()\n",
    "    return (\n",
    "        db_table.search(query_vector)\n",
    "        .limit(max_results)\n",
    "        .select(\n",
    "            [\n",
    "                \"release_year\",\n",
    "                \"title\",\n",
    "                \"origin\",\n",
    "                \"director\",\n",
    "                \"cast\",\n",
    "                \"genre\",\n",
    "                \"plot\",\n",
    "                \"_distance\",\n",
    "            ]\n",
    "        )\n",
    "        .to_list()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What should I see tonight? I love Sci-Fi movies but I have seen most of the classics, such as Star Wars.\"\n",
    "\n",
    "results = get_records(question, max_results=3)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "This is the R part in RAG: let's take care of the G, shall we?\n",
    "\n",
    "In the following cells we construct the system message and the prompt of our movie expert. Of course, the prompt must include the movies retrieved by the vector store as context for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEEK_SYSTEM = \"\"\"\n",
    "  You are a DVD record store assistant and your goal is to recommed the user with a good movie to watch.\n",
    "\n",
    "  You are a movie expert and a real geek: you love sci-fi movies and tend to get excited when you talk about them.\n",
    "  Nevertheless, no matter what, you always want to make your customers happy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Here are some suggested movies (ranked by relevance) to help you with your choice.\n",
    "  {context}\n",
    "\n",
    "  Use these suggestions to answer this question:\n",
    "  {question}\n",
    "\"\"\"\n",
    "\n",
    "context_template = \"\"\"\n",
    "Title: {title}\n",
    "Release date: {release_year}\n",
    "Director: {director}\n",
    "Cast: {cast}\n",
    "Genre: {genre}\n",
    "Overview: {plot}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_records_into_context(records, *, template):\n",
    "    return \"\".join(\n",
    "        context_template.format(\n",
    "            title=rec[\"title\"],\n",
    "            release_year=rec[\"release_year\"],\n",
    "            director=rec[\"director\"],\n",
    "            cast=rec[\"cast\"],\n",
    "            genre=rec[\"genre\"],\n",
    "            plot=rec[\"plot\"],\n",
    "        )\n",
    "        for rec in results\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Everything ready to ask a LLM to reply the user questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "def ask(\n",
    "    question,\n",
    "    *,\n",
    "    max_results=10,\n",
    "    system=GEEK_SYSTEM,\n",
    "    prompt_template=prompt_template,\n",
    "    context_template=context_template,\n",
    "    db_table=movies_table,\n",
    "    verbose=False,\n",
    "):\n",
    "    records = get_records(\n",
    "        query=question,\n",
    "        max_results=max_results,\n",
    "        db_table=movies_table,\n",
    "    )\n",
    "    context = format_records_into_context(records, template=context_template)\n",
    "\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = chat_completion\n",
    "    if verbose:\n",
    "        print(\n",
    "            answer.choices[0].message.content,\n",
    "            \"=== CONTEXT ===\",\n",
    "            context,\n",
    "        )\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = ask(question=question, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The function `ask` is our product. Yes, we could wrap in in a API using a framework like [FastAPI](https://duckduckgo.com/?q=fastapi&ia=web), or chat interface with [Chainlit](https://docs.chainlit.io/get-started/overview). But let's keep things simple.\n",
    "\n",
    "There is something else that matters - something more pressing: **is this reply good?** Well, it looks good. It replies in a funny tone and suggests a movie! Maybe it's too long... or too short?\n",
    "\n",
    "Here's the true goal of the workshop, and what we will do next.\n",
    "\n",
    "Until the break, we'll go over the second and third notebook of the series. We will introduce *subject matter experts* (SME) and how we can leverage their knowledge to shape the product and evaluate it. We'll involve the SME to **\"bootstrap\" an evaluation dataset**, and use their comments and critiques to build a LLM that **reflects their preferences** and will help us with **evaluating the interactions at scale**. This pattern is known in the literature as \"LLM-as-a-judge\" [(arXiv:2306.05685)](https://arxiv.org/abs/2306.05685). For the implementation, we'll follow [this excellent article](https://hamel.dev/blog/posts/llm-judge/) by Hamel Husain, who's an independent AI consultant and a part time researcher at Answer.AI, a lab directed by Jeremy Howard.\n",
    "\n",
    "In the second part, we'll implement two pattenrs to enhance our RAG chatbot: Hypothetical Document Extraction (HyDE) [(arXiv:2212.10496)](https://arxiv.org/abs/2212.10496) and metadata extraction/filtering to make our queries more precise. While not fancy, these will be the first tools you'll likely use to improve your RAG application (outside chunking).\n",
    "\n",
    "Keep this page open: we'll come back to the last section of the notebook later.\n",
    "\n",
    "It's time to move to the second notebook: [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xtreamsrl/movies-buddy/blob/main/notebooks/02-domain-expert.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Let's Run Our First Evaluation Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now that we have generated some questions to bootstrap our evaluation process, we must make our system reply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "PL_STR_LEN = 1500\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(PL_STR_LEN)\n",
    "pl.Config.set_tbl_width_chars(PL_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = pl.read_csv(\"https://raw.githubusercontent.com/xtreamsrl/beyond-the-hype/refs/heads/main/data/eval_questions.csv\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beyond_the_hype.judge import answer_multiple_questions\n",
    "\n",
    "replied_questions = answer_multiple_questions(eval_dataset, ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.with_columns(\n",
    "    pl.col(\"question\")\n",
    "    .map_elements(\n",
    "        lambda question: ask(question).choices[0].message.content,\n",
    "        return_dtype=pl.String,\n",
    "    )\n",
    "    .alias(\"rag_answer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "replied_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Cool! Our movie expert replies to all the questions in our dataset; our work here is done! But... let's see what our **movie expert** would say about it.\n",
    "\n",
    "He's not accustomed to `polars`, so let's prepare a more convenient Excel for him!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Could we improve this Excel a little bit later\n",
    "eval_dataset.write_excel(\n",
    "    workbook=\"../data/eval_questions_answered.xlsx\", column_totals=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Let's see what the domain expert voted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques = pl.read_excel(\n",
    "    source=\"../data/eval_questions_with_critiques.xlsx\",\n",
    "    sheet_name=\"Sheet1\",\n",
    ")\n",
    "domain_expert_critiques.with_columns(pl.col(\"Judgement\").cast(pl.Categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques.select(\n",
    "    pl.col(\"Judgement\").value_counts(sort=True, name=\"Count\")\n",
    ").unnest(\"Judgement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "There are a lot of failures (80%), so let's eye-balling the critiques from the domain expert and cluster them in groups. Possible groups could be:\n",
    "- missing context: frequently, domain experts said that the model replied saying that it doesn't have the movie in the list\n",
    "- The system always gives the same suggestion (\"Welcome to the Space Show\")\n",
    "- It doesn't manage corner cases, such as unrelated or toxic questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## The \"Easiest First\" Rule\n",
    "Don't panic! You don't need to rebuild all your RAG. Let's keep things simple before discussing complex considerations about restructuring your RAG architecture.\n",
    "\n",
    "The easiest things to look at are prompts and system messages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GEEK_SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Let's quickly iterate through our system message to check if we could have a better response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\" You are a movie expert, and your goal is to recommend the user with a good movie to watch.\n",
    "\n",
    "RULES: \n",
    "- You should reply to questions about: movies plots or synopsys, movies metadata (release date, cast, or director), provide plots summary;\n",
    "- For every questions outside the scope please reply politely that you're not able to provide a response and describe briefly your scope;\n",
    "- Don't mention that you have a list of films as a context. This should be transparent to the user\n",
    "- If you don't have the movie in your context reply that you don't know how to reply\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.with_columns(\n",
    "    pl.col(\"question\")\n",
    "    .map_elements(\n",
    "        lambda question: ask(question, system=SYSTEM_MESSAGE)\n",
    "        .choices[0]\n",
    "        .message.content,\n",
    "        return_dtype=pl.String,\n",
    "    )\n",
    "    .alias(\"rag_answer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
