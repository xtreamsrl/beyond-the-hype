{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## You Need A Subject Matter Expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We need to evaluate our AI product. I saw you googling (or chatGPTing) for the best metric to assess your RAG. *Stop it immediately!* A typical error when building an evaluation system for an RAG is adopting many confusing metrics. **It is hard to find a quality response metric in a domain with open-ended responses, especially at the beginning.**\n",
    "\n",
    "Before digging into fancy metrics, **you must involve Subject Matters Experts (SMEs)** in the project to build a successful AI product. Someone who knows everything about the domain and will use your product or is interested in creating a helpful product: If you are building an AI system that needs to reply to new employee questions about the onboarding process, you can involve an HR manager. Again, involve a lawyer if your product must respond to legal questions.\n",
    "\n",
    "It's even better if you can identify the *principal* subject matter expert. As Hamel says, the principal SME is someone \"whose judgment is crucial for the success of your AI product. These are the people with deep domain expertise or represent your target users.\" There are a couple of reasons why you should find them: they set the standard and, since they are at most two people, their judgment will be consistent. Also, their involvement might make them feel owners of the projects.\n",
    "\n",
    "By now, you should get the idea: since we are building a movie expert, we need a cinema geek! Yes, it might be hard to find one in the room. But perhaps you can turn to the person sitting next to you, and treat them as your expert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### How to engage the SME, synthetic data, and the data flywheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now that we have chosen our SME, we need to swap our developer hat and start thinking as a product designer. How can we enage the experts? The first thing that comes to mind is that we could ask them to come up with example/ideal interactions. This might take time, and might not cover all of the ideal use cases.\n",
    "\n",
    "There's instead something else we could ask them to: judge a set of questions and answers we already have. If you think about it, it's much harder for anyone of us to write a poem; on the other hand, it's much easier to say if a poem is simply good or bad. We can leverage the power of LLM to come up with both the questions and answers to bootstrap our evaluation pipeline.\n",
    "\n",
    "Two caveats:\n",
    "\n",
    "1. Synthetic data might not cover every edge case - that's a fact. But, keep in mind, we are trying to go from zero to one here. Once your product is online, you should set up an observability framework to monitor the interactions, and figure out a way to include real-world interactions in your eval datasets (that are compliant with data regulation and policies). This is a powerful pattern, named *data flywheel*. See it [here](https://jxnl.co/writing/2024/03/28/data-flywheel/) and [here](https://hamel.dev/blog/posts/evals/#problem-how-to-systematically-improve-the-ai).\n",
    "2. You might want to use the most powerful model you have at your disposal for this. Evals are expensive. But making changes to your product without them might cost you even more. Also, it will still likely be cheaper than having your colleagues coming up with the bootstrap data themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Build the first eval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Features, Scenarios and Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The goal of this stage is to keep a tight and fast feedback loop with the SME. At this step, we need to uncover the expectations and prioritise the features. A workshop might be an ideal setting for this, though you might not always find the time. So here's a simple framework that might be of help to start bootstrapping your evals, focussing on the different characteristics that our product must have.\n",
    "\n",
    "- Which **features** should our system have? What must it must do specifically?\n",
    "- Which **scenarios** should it be able to handle?\n",
    "- Which kind of **users**  will use the product. Is it expert users, technical or non-technical people?\n",
    "\n",
    "Every combination of feature-scenario-user could be an entry in your eval dataset. To keep computing times short, we will just start with generating 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Exercise: Come up with Features, Scenarios, and Users\n",
    "\n",
    "Here you will find a list of possible features, scenarsios and personas for the movie buddy. Do you agree with them? Feel free to amend them, improve the formulation, or add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_expert_features = [\n",
    "    \"Movies Recommendation\",\n",
    "    \"Movies Synopsys\",\n",
    "    \"Movie Metadata (cast, director, release dates)\",\n",
    "]\n",
    "movie_expert_scenarios = [\n",
    "    \"Generic questions without details\",\n",
    "    \"Question non related to movies\",\n",
    "    \"Toxic Questions\",\n",
    "]\n",
    "movie_expert_personas = [\n",
    "    \"Movie entushiast\",\n",
    "    \"New Users\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Generate the eval dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now we need to generate questions for each triplet. For this, we leverage some basic prompt engineering, and structured output generation ([arXiv:2307.09702](https://arxiv.org/abs/2307.09702))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"Act as you are a AI system tester. \n",
    "The user is a domain expert that must evaluate the answer generated by an AI system of your questions. \n",
    "Your role is to generate a dataset of questions to test a movie expert AI system. Note that the questions could be vary and follow \n",
    "\n",
    "RULES:\n",
    "- The questions should test ONLY the following product features: {features}.\n",
    "- The questions should test ONLY the following usage scenarios: {scenarios}\n",
    "- You must generate the questions impersonating ONLY the following personas: {personas}\n",
    "\"\"\"\n",
    "\n",
    "MAX_ROWS = 10\n",
    "EVAL_CONSTRUCTION_PROMPT = (\n",
    "    \"\"\"Generate an evaluation dataset with no more than {n_rows} rows\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beyond_the_hype.synthetic import build_eval_dataset_builder_system_message\n",
    "\n",
    "system_message = build_eval_dataset_builder_system_message(\n",
    "    SYSTEM_MESSAGE,\n",
    "    movie_expert_features,\n",
    "    movie_expert_scenarios,\n",
    "    movie_expert_personas,\n",
    ")\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "import polars as pl\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class EvalQuestionFormat(TypedDict):\n",
    "    question_id: int\n",
    "    question: str\n",
    "    feature: str\n",
    "    scenario: str\n",
    "    persona: str\n",
    "\n",
    "\n",
    "class EvalDataset(BaseModel):\n",
    "    questions: list[EvalQuestionFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "answer = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": EVAL_CONSTRUCTION_PROMPT.format(n_rows=MAX_ROWS)},\n",
    "    ],\n",
    "    response_format=EvalDataset,\n",
    ")\n",
    "\n",
    "answer = answer.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.model_dump()[\"questions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now, we have a list of questions to pose to our AI and ask our evaluation expert to evaluate it. Note that, the SME could be involved also both for giving you features, scenario and personas or to add particular questions to the generated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pl.from_dicts(answer.model_dump()[\"questions\"])\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now let's save this locally. Downlaod it and head back to the first notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.write_csv(\"./eval_questions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
