{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HyDE: Hypothetical Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In traditional vector search, queries are converted into embeddings and compared with a database of stored embeddings. HyDE enhances retrieval by first generating a hypothetical response and embedding that instead of the raw query. This helps in cases where:\n",
    "\n",
    "- Queries are ambiguous or too short\n",
    "- There isn't a direct match in the vector database\n",
    "- LLMs can generate useful contextual information before retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install some code utilities\n",
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec(\"beyond_the_hype\"):\n",
    "    !pip install -qqq git+https://github.com/xtreamsrl/beyond-the-hype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beyond_the_hype.data import get_movies_dataset\n",
    "from beyond_the_hype.judge import llm_as_a_judge, answer_multiple_questions\n",
    "\n",
    "import openai\n",
    "import polars as pl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = get_movies_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"./data/movies_embeddings\"\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "movies_table = db.create_table(\"movies\", movies, mode=\"overwrite\")\n",
    "\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "HyDE is composed of two main steps:\n",
    "- Generate a hypothetical document: we ask an LLM to generate a document that could reply to a given question. This document is called **\"hypothetical\"** because it's not real and could contain factual errors or hallucinations, but it looks like an actual document and could help retrieve documents.\n",
    "- Use the hypothetical document (instead of the user question) to search between vectors.\n",
    "\n",
    "The embedding encoder works as a lossy compressor that filters away all extra things, including errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyde_query(client: openai.OpenAI, query: str) -> str:\n",
    "    hyde_prompt = f\"\"\"\n",
    "You are the best movie expert on the market. \n",
    "Generate a document that could be used to reply the following question:\n",
    "{query}\n",
    "Give just the document. Don't add unnecessary information such as title etc. \n",
    "    \"\"\"\n",
    "    hyde_fake_reply = (\n",
    "        client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": hyde_prompt},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    return hyde_fake_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How The wolf of wall street end?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hyde_query(client, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(query, *, encoder=encoder, db_table=movies_table, max_results=10):\n",
    "    query_vector = encoder.encode(query).tolist()\n",
    "    return (\n",
    "        db_table.search(query_vector)\n",
    "        .limit(10)\n",
    "        .select(\n",
    "            [\n",
    "                \"release_year\",\n",
    "                \"title\",\n",
    "                \"origin\",\n",
    "                \"director\",\n",
    "                \"cast\",\n",
    "                \"genre\",\n",
    "                \"plot\",\n",
    "                \"_distance\",\n",
    "            ]\n",
    "        )\n",
    "        .to_list()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\" You are a movie expert, and your goal is to recommend the user with a good movie to watch.\n",
    "\n",
    "RULES: \n",
    "- You should reply to questions about: movies plots or synopsys, movies metadata (release date, cast, or director), provide plots summary;\n",
    "- For every questions outside the scope please reply politely that you're not able to provide a response and describe briefly your scope;\n",
    "- Don't mention that you have a list of films as a context. This should be transparent to the user\n",
    "- If you don't have the movie in your context reply that you don't know how to reply\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Here are some suggested movies (ranked by relevance) to help you with your choice.\n",
    "  {context}\n",
    "\n",
    "  Use these suggestions to answer this question:\n",
    "  {question}\n",
    "\"\"\"\n",
    "\n",
    "context_template = \"\"\"\n",
    "Title: {title}\n",
    "Release date: {release_year}\n",
    "Director: {director}\n",
    "Cast: {cast}\n",
    "Genre: {genre}\n",
    "Overview: {plot}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_records_into_context(records, *, template):\n",
    "    return \"\".join(\n",
    "        context_template.format(\n",
    "            title=rec[\"title\"],\n",
    "            release_year=rec[\"release_year\"],\n",
    "            director=rec[\"director\"],\n",
    "            cast=rec[\"cast\"],\n",
    "            genre=rec[\"genre\"],\n",
    "            plot=rec[\"plot\"],\n",
    "        )\n",
    "        for rec in records\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    question,\n",
    "    *,\n",
    "    max_results=10,\n",
    "    system=SYSTEM_MESSAGE,\n",
    "    prompt_template=prompt_template,\n",
    "    context_template=context_template,\n",
    "    db_table=movies_table,\n",
    "    verbose=False,\n",
    "):\n",
    "    fake_hyde_reply = create_hyde_query(client, question)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"FAKE HYDE REPLY:\\n{fake_hyde_reply}\\n\\n\")\n",
    "\n",
    "    results = get_records(\n",
    "        query=fake_hyde_reply, max_results=max_results, db_table=movies_table\n",
    "    )\n",
    "    context = format_records_into_context(results, template=context_template)\n",
    "\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = chat_completion\n",
    "    if verbose:\n",
    "        print(f\"RETRIEVED CONTEXT: \\n{context}\\n\\n\")\n",
    "        print(f\"FINAL REPLY:\\n{answer.choices[0].message.content}\\n\\n\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = ask(question=question, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# But... Is Our RAG Improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's take our questions/answers dataset and run again our LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers_df = pl.read_csv(\n",
    "    source=\"eval_replies.csv\"\n",
    ").select([\"question\", \"rag_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "replied_answers = answer_multiple_questions(questions_answers_df, ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "judged_questions_answer_df = llm_as_a_judge(questions_answers_df, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "judged_questions_answer_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
