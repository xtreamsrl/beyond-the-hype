{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Let's Build an LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In the previous sections, we argued that having a domain expert is fundamental to the success of your AI product. However, involving a domain expert every time you need an evaluation could be costly economically and in terms of time.\n",
    "\n",
    "AI help us again! \n",
    "\n",
    "We could ask an LLM to judge how our movie expert answers a set of questions. It will mimic the domain expert in giving us an outcome and proposing a textual critique. Note that a domain expert remains fundamental to understand what it is expected from the product and his considerations should guide us in building a strong LLM-as-a-judge tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's build everything we need to ask an LLM to judge our questions. Starting from a system message, we provide two elements to the system message:\n",
    "- **guidelines** to instruct how we expect the evaluations and,\n",
    "- **examples** to improve the quality of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExample(BaseModel):\n",
    "    question: str\n",
    "    response: str\n",
    "    outcome: str\n",
    "    critique: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_MESSAGE = \"\"\"You are a movie expert chatbot evaluator with advanced capabilities to understand if the question response is good or not.\n",
    "Follow some guidelines to evaluate the system:\n",
    "{guidelines}\n",
    "\n",
    "We provide you with some examples to give you an idea of how a good response should be.\n",
    "Evaluation examples:\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE_STRUCTURE = \"\"\"<user_question>{question}</user_question>\n",
    "<response>{response}</response>\n",
    "<outcome>{outcome}</outcome>\n",
    "<critique>{critique}</critique>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_guidelines = [\n",
    "    'Provide a \"pass\" or \"fail\" outcome. No other category is admitted.'\n",
    "    'Provide a comprehensive and clear explanation of why you gave that outcome. In particular for the \"fail\" one'\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    EvaluationExample(\n",
    "        question=\"Could you suggest an action movie?\",\n",
    "        response='I recommend \"The Thousand Faces of Dunjia\" (2017), directed by Yuen Woo-ping. The film follows a group of swordsmen\\'s adventures as they secretly protect humanity by hunting down mysterious creatures from outer space. It combines action and fantasy elements, making it an engaging watch for fans of the genre. Enjoy!',\n",
    "        outcome=\"pass\",\n",
    "        critique=\"The system replied with an action movie correctly. The response is brief but well explained.\",\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        question=\"Give me a summary of DUNE II\",\n",
    "        response=\"I'm sorry, but I don't have information about \\\"Dune II.\\\" My expertise covers movie plots, metadata, and summaries of films, but it seems that title isn't available in my current context. If you have any questions about other movies, feel free to ask!\",\n",
    "        outcome=\"fail\",\n",
    "        critique=\"The system should reply to questions regarding movie summaries. It seems that can't find the movie in the context.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We put all together in a single string to obtain the final formatted system message (`formatted_system_message`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluation_examples(example: EvaluationExample) -> str:\n",
    "    return EXAMPLE_STRUCTURE.format(\n",
    "        question=example.question,\n",
    "        response=example.response,\n",
    "        outcome=example.outcome,\n",
    "        critique=example.critique,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_evaluation_system_message(\n",
    "    system_message: str,\n",
    "    guidelines: str,\n",
    "    examples: list[EvaluationExample],\n",
    ") -> str:\n",
    "    formatted_guidelines = \"\\n\".join(guidelines)\n",
    "    formatted_examples = \"\\n\\n\".join([build_evaluation_examples(ex) for ex in examples])\n",
    "    return system_message.format(\n",
    "        guidelines=formatted_guidelines, examples=formatted_examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_message = build_evaluation_system_message(\n",
    "    JUDGE_SYSTEM_MESSAGE,\n",
    "    evaluation_guidelines,\n",
    "    examples,\n",
    ")\n",
    "print(formatted_system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now, we need to ask to judge a list of questions and response couples. We could start looking at the questions already evaluated by our domain expert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques = pl.read_csv(\"eval_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_questions = (\n",
    "    domain_expert_critiques.select(\n",
    "        pl.format(\n",
    "            \"Question: {}\\nAnswer: {}\\n\\n\", pl.col(\"question\"), pl.col(\"rag_answer\")\n",
    "        )\n",
    "    )\n",
    "    .to_series(0)\n",
    "    .str.join(\"\\n\")\n",
    "    .item()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Evaluate how our AI system answered the given questions.\n",
    "Here is the list of question and answer couples: \n",
    "{formatted_questions}\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudgeDataset(BaseModel):\n",
    "    critiques: list[EvaluationExample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": formatted_system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=LLMJudgeDataset,\n",
    ")\n",
    "\n",
    "llm_judge_outcome = chat_completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure polars to display more text in each row\n",
    "PL_STR_LEN = 1000\n",
    "_ = pl.Config.set_fmt_str_lengths(PL_STR_LEN)\n",
    "_ = pl.Config.set_tbl_width_chars(PL_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df = pl.from_dicts(llm_judge_outcome.model_dump()[\"critiques\"])\n",
    "llm_judge_df = llm_judge_df.rename(\n",
    "    {\"outcome\": \"Judgement_llm_judge\", \"critique\": \"Critique_llm_judge\"}\n",
    ")\n",
    "llm_judge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## How does our evaluator perform compared to domain expert?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Domain Expert outcomes and critiques are still essential! To determine the performance of the LLM-as-a-judge tool we build, we should compare their response with the one from our domain expert. **Domain expert responses are our ground truth.** The ideal result that we could expect is that the automatic evaluator aligns with human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges = pl.concat(\n",
    "    [\n",
    "        domain_expert_critiques,\n",
    "        llm_judge_df.select([\"Judgement_llm_judge\", \"Critique_llm_judge\"]),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## üèãüèª Exercise: Iterate until LLM-as-a-Judge aligns with the expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Go back to the prompt and system message, try to edit it, and rerun the code until it aligns with the results of the domain expert! \n",
    "\n",
    "You could also involve the domain expert again (who should be beside you) to check if the LLM-as-a-judge answer stimulated his change of mind. The results might surprise you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
