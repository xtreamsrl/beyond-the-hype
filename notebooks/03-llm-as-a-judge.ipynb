{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Let's Build an LLM-as-a-Judge Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In the previous sections, we argued that having a domain expert is fundamental to the success of your AI product. However, involving a domain expert every time you need an evaluation could be costly economically and in terms of time.\n",
    "\n",
    "AI help us again! \n",
    "\n",
    "We could ask an LLM to judge how our movie expert answers a set of questions. It will mimic the domain expert in giving us an outcome and proposing a textual critique. Note that a domain expert remains fundamental to understand what it is expected from the product and his considerations should guide us in building a strong LLM-as-a-judge tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's build everything we need to ask an LLM to judge our questions. Starting from a system message, we provide two elements to the system message:\n",
    "- **guidelines** to instruct how we expect the evaluations and,\n",
    "- **examples** to improve the quality of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExample(BaseModel):\n",
    "    question: str\n",
    "    response: str\n",
    "    outcome: str\n",
    "    critique: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_MESSAGE = \"\"\"You are a movie expert chatbot evaluator with advanced capabilities to understand if the question response is good or not.\n",
    "Follow some guidelines to evaluate the system:\n",
    "{guidelines}\n",
    "\n",
    "We provide you with some examples to give you an idea of how a good response should be.\n",
    "Evaluation examples:\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE_STRUCTURE = \"\"\"<user_question>{question}</user_question>\n",
    "<response>{response}</response>\n",
    "<outcome>{outcome}</outcome>\n",
    "<critique>{critique}</critique>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_guidelines = [\n",
    "    'Provide a \"pass\" or \"fail\" outcome. No other category is admitted.'\n",
    "    'Provide a comprehensive and clear explanation of why you gave that outcome. In particular for the \"fail\" one'\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    EvaluationExample(\n",
    "        question=\"Could you suggest an action movie?\",\n",
    "        response='I recommend \"The Thousand Faces of Dunjia\" (2017), directed by Yuen Woo-ping. The film follows a group of swordsmen\\'s adventures as they secretly protect humanity by hunting down mysterious creatures from outer space. It combines action and fantasy elements, making it an engaging watch for fans of the genre. Enjoy!',\n",
    "        outcome=\"pass\",\n",
    "        critique=\"The system replied with an action movie correctly. The response is brief but well explained.\",\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        question=\"Give me a summary of DUNE II\",\n",
    "        response=\"I'm sorry, but I don't have information about \\\"Dune II.\\\" My expertise covers movie plots, metadata, and summaries of films, but it seems that title isn't available in my current context. If you have any questions about other movies, feel free to ask!\",\n",
    "        outcome=\"fail\",\n",
    "        critique=\"The system should reply to questions regarding movie summaries. It seems that can't find the movie in the context.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We put all together in a single string to obtain the final formatted system message (`formatted_system_message`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluation_examples(example: EvaluationExample) -> str:\n",
    "    return EXAMPLE_STRUCTURE.format(\n",
    "        question=example.question,\n",
    "        response=example.response,\n",
    "        outcome=example.outcome,\n",
    "        critique=example.critique,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_evaluation_system_message(\n",
    "    system_message: str, guidelines: str, examples: list[EvaluationExample]\n",
    ") -> str:\n",
    "    formatted_guidelines = \"\\n\".join(guidelines)\n",
    "    formatted_examples = \"\\n\\n\".join([build_evaluation_examples(ex) for ex in examples])\n",
    "    return system_message.format(\n",
    "        guidelines=formatted_guidelines, examples=formatted_examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_message = build_evaluation_system_message(\n",
    "    JUDGE_SYSTEM_MESSAGE, evaluation_guidelines, examples\n",
    ")\n",
    "print(formatted_system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now, we need to ask to judge a list of questions and response couples. We could start looking at the questions already evaluated by our domain expert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques = pl.read_excel(\n",
    "    source=\"../data/eval_questions_with_critiques.xlsx\",\n",
    "    sheet_name=\"Sheet1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_questions = (\n",
    "    domain_expert_critiques.select(\n",
    "        pl.format(\n",
    "            \"Question: {}\\nAnswer: {}\\n\\n\", pl.col(\"question\"), pl.col(\"rag_answer\")\n",
    "        )\n",
    "    )\n",
    "    .to_series(0)\n",
    "    .str.join(\"\\n\")\n",
    "    .item()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Evaluate how our AI system answered the given questions.\n",
    "Here is the list of question and answer couples: \n",
    "{formatted_questions}\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudgeDataset(BaseModel):\n",
    "    critiques: list[EvaluationExample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": formatted_system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=LLMJudgeDataset,\n",
    ")\n",
    "\n",
    "llm_judge_outcome = chat_completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df = pl.from_dicts(llm_judge_outcome.model_dump()[\"critiques\"])\n",
    "llm_judge_df = llm_judge_df.rename(\n",
    "    {\"outcome\": \"Judgement_llm_judge\", \"critique\": \"Critique_llm_judge\"}\n",
    ")\n",
    "llm_judge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Now, we can compare the judgement of the domain expert with the judgement from the llm-as-a-judge and, if needed, iterate on the tool to align it to the human one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges = pl.concat(\n",
    "    [\n",
    "        domain_expert_critiques,\n",
    "        llm_judge_df.select([\"Judgement_llm_judge\", \"Critique_llm_judge\"]),\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
